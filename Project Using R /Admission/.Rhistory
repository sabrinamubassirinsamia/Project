skim_without_charts(df_c)
df_c %>% count(diagnosis)
# setting diagnosis column to factor for plotting and modeling
df_c$diagnosis <- as.factor(df_c$diagnosis)
# plotting the bar plot
ggplot(df_c, aes(x=diagnosis, fill= diagnosis)) +
geom_bar(stat="count") +
theme_classic() +
scale_y_continuous(breaks = seq(0, 400, by = 25)) +
labs(title="Distribution of Diagnosis")
fig2 <- df_c[c("radius_mean", "diagnosis")]
# plotting the box plot
ggplot(fig2, aes(diagnosis, radius_mean, fill = diagnosis)) +
geom_boxplot()+
labs(col="Type of The Tumor") + ylab("lobes radius mean") +
labs(title="Distribution of diagnosis")+
# changing the color of the boxplots
scale_fill_manual(values = c( "dodgerblue1","red3"))
ggplot(df_c, aes(radius_mean, concavity_mean)) +
geom_point(aes(color = diagnosis)) +
labs(title = "Radious mean Vs Concavity mean",
y = "Mean of Concavity", x = "Radius of Lobes",
col="Type of The Tumor")+
scale_colour_manual(labels = c("Benign", "Malignant"),
values = c("dodgerblue1","red2")) +
theme_bw()
#Machine Learning Analysis
#Splitting the data into training set and test set
# setting the independent variable to factor so that we can be able to fit it to the classification model
df_c$diagnosis <- as.factor(df_c$diagnosis)
df_c <- df_c[,-1] #Removing ID column as it does not have correlation
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_c$diagnosis, p=split, list=FALSE)
data_train <- df_c[ trainIndex,]
data_test <- df_c[-trainIndex,]
#Building a Cross validation Object
# Convert the 'diagnosis' column to a factor (if not already)
data_train$diagnosis <- as.factor(data_train$diagnosis)
# Impute missing values using k-NN imputation
preProcessParams <- preProcess(data_train, method = "knnImpute")
data_train <- predict(preProcessParams, newdata = data_train)
library(tidyverse) #For tidying the data
library(skimr) #It is designed to provide summary statistics about variables in data frames
library(ggplot2) #For plotting
library(RColorBrewer) #For aesthetics
library(tidymodels) #For splitting the data and training
library(caret) #For machine learning
library(party) #The core of the package is ctree()
library(yardstick) #For plotting the confusion matrix
df <- read.csv("/Users/sabrinamobassirin/Downloads/Fall23/Stat5301_Applied Regression/Project/Brest_Cancer_prediction/breastcancer.csv")
dim(df)
names(df)
str(df)
summary(df)
any(is.na(df))
colnames(df)
colSums(is.na(df))
# Replacing the values of these columns by the mean
df$radius_mean[is.na(df$radius_mean)] <- mean(df$radius_mean, na.rm = TRUE)
df$concave.points_mean[is.na(df$concave.points_mean)] <- mean(df$concave.points_mean,na.rm = TRUE)
df$smoothness_se[is.na(df$smoothness_se)] <- mean(df$smoothness_se, na.rm = TRUE)
df$texture_worst[is.na(df$texture_worst)] <- mean(df$texture_worst, na.rm = TRUE)
df$symmetry_worst[is.na(df$symmetry_worst)] <- mean(df$symmetry_worst, na.rm = TRUE)
# checking again if there is any NA values in the numeric columns
colSums(is.na(df))
# the only columns that is character is "diagnosis"
# this is a function to find the mode in a column
calc_mode <- function(x)
{
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
# replacing the NA values by the mode
df <- df %>%
mutate(diagnosis = if_else(is.na(diagnosis),
calc_mode(diagnosis),
diagnosis))
# checking again for NA values in the data.
any(is.na(df)) #Should Be FALSE
df_c <-df
df_c
skim_without_charts(df_c)
df_c %>% count(diagnosis)
# setting diagnosis column to factor for plotting and modeling
df_c$diagnosis <- as.factor(df_c$diagnosis)
# plotting the bar plot
ggplot(df_c, aes(x=diagnosis, fill= diagnosis)) +
geom_bar(stat="count") +
theme_classic() +
scale_y_continuous(breaks = seq(0, 400, by = 25)) +
labs(title="Distribution of Diagnosis")
fig2 <- df_c[c("radius_mean", "diagnosis")]
# plotting the box plot
ggplot(fig2, aes(diagnosis, radius_mean, fill = diagnosis)) +
geom_boxplot()+
labs(col="Type of The Tumor") + ylab("lobes radius mean") +
labs(title="Distribution of diagnosis")+
# changing the color of the boxplots
scale_fill_manual(values = c( "dodgerblue1","red3"))
ggplot(df_c, aes(radius_mean, concavity_mean)) +
geom_point(aes(color = diagnosis)) +
labs(title = "Radious mean Vs Concavity mean",
y = "Mean of Concavity", x = "Radius of Lobes",
col="Type of The Tumor")+
scale_colour_manual(labels = c("Benign", "Malignant"),
values = c("dodgerblue1","red2")) +
theme_bw()
#Machine Learning Analysis
library(kernlab)
#Splitting the data into training set and test set
# setting the independent variable to factor so that we can be able to fit it to the classification model
df_c$diagnosis <- as.factor(df_c$diagnosis)
df_c <- df_c[,-1] #Removing ID column as it does not have correlation
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_c$diagnosis, p=split, list=FALSE)
data_train <- df_c[ trainIndex,]
data_test <- df_c[-trainIndex,]
#Building a Cross validation Object
# Convert the 'diagnosis' column to a factor (if not already)
data_train$diagnosis <- as.factor(data_train$diagnosis)
# Impute missing values using k-NN imputation
preProcessParams <- preProcess(data_train, method = "knnImpute")
data_train <- predict(preProcessParams, newdata = data_train)
install.packages("RANN")
library(tidyverse) #For tidying the data
library(skimr) #It is designed to provide summary statistics about variables in data frames
library(ggplot2) #For plotting
library(RColorBrewer) #For aesthetics
library(tidymodels) #For splitting the data and training
library(caret) #For machine learning
library(party) #The core of the package is ctree()
library(yardstick) #For plotting the confusion matrix
df <- read.csv("/Users/sabrinamobassirin/Downloads/Fall23/Stat5301_Applied Regression/Project/Brest_Cancer_prediction/breastcancer.csv")
dim(df)
names(df)
str(df)
summary(df)
any(is.na(df))
colnames(df)
colSums(is.na(df))
# Replacing the values of these columns by the mean
df$radius_mean[is.na(df$radius_mean)] <- mean(df$radius_mean, na.rm = TRUE)
df$concave.points_mean[is.na(df$concave.points_mean)] <- mean(df$concave.points_mean,na.rm = TRUE)
df$smoothness_se[is.na(df$smoothness_se)] <- mean(df$smoothness_se, na.rm = TRUE)
df$texture_worst[is.na(df$texture_worst)] <- mean(df$texture_worst, na.rm = TRUE)
df$symmetry_worst[is.na(df$symmetry_worst)] <- mean(df$symmetry_worst, na.rm = TRUE)
# checking again if there is any NA values in the numeric columns
colSums(is.na(df))
# the only columns that is character is "diagnosis"
# this is a function to find the mode in a column
calc_mode <- function(x)
{
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
# replacing the NA values by the mode
df <- df %>%
mutate(diagnosis = if_else(is.na(diagnosis),
calc_mode(diagnosis),
diagnosis))
# checking again for NA values in the data.
any(is.na(df)) #Should Be FALSE
df_c <-df
df_c
skim_without_charts(df_c)
df_c %>% count(diagnosis)
# setting diagnosis column to factor for plotting and modeling
df_c$diagnosis <- as.factor(df_c$diagnosis)
# plotting the bar plot
ggplot(df_c, aes(x=diagnosis, fill= diagnosis)) +
geom_bar(stat="count") +
theme_classic() +
scale_y_continuous(breaks = seq(0, 400, by = 25)) +
labs(title="Distribution of Diagnosis")
fig2 <- df_c[c("radius_mean", "diagnosis")]
# plotting the box plot
ggplot(fig2, aes(diagnosis, radius_mean, fill = diagnosis)) +
geom_boxplot()+
labs(col="Type of The Tumor") + ylab("lobes radius mean") +
labs(title="Distribution of diagnosis")+
# changing the color of the boxplots
scale_fill_manual(values = c( "dodgerblue1","red3"))
ggplot(df_c, aes(radius_mean, concavity_mean)) +
geom_point(aes(color = diagnosis)) +
labs(title = "Radious mean Vs Concavity mean",
y = "Mean of Concavity", x = "Radius of Lobes",
col="Type of The Tumor")+
scale_colour_manual(labels = c("Benign", "Malignant"),
values = c("dodgerblue1","red2")) +
theme_bw()
#Machine Learning Analysis
library(kernlab)
#Splitting the data into training set and test set
# setting the independent variable to factor so that we can be able to fit it to the classification model
df_c$diagnosis <- as.factor(df_c$diagnosis)
df_c <- df_c[,-1] #Removing ID column as it does not have correlation
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_c$diagnosis, p=split, list=FALSE)
data_train <- df_c[ trainIndex,]
data_test <- df_c[-trainIndex,]
#Building a Cross validation Object
# Convert the 'diagnosis' column to a factor (if not already)
data_train$diagnosis <- as.factor(data_train$diagnosis)
# Impute missing values using k-NN imputation
preProcessParams <- preProcess(data_train, method = "knnImpute")
data_train <- predict(preProcessParams, newdata = data_train)
library(tidyverse) #For tidying the data
library(skimr) #It is designed to provide summary statistics about variables in data frames
library(ggplot2) #For plotting
library(RColorBrewer) #For aesthetics
library(tidymodels) #For splitting the data and training
library(caret) #For machine learning
library(party) #The core of the package is ctree()
library(yardstick) #For plotting the confusion matrix
df <- read.csv("/Users/sabrinamobassirin/Downloads/Fall23/Stat5301_Applied Regression/Project/Brest_Cancer_prediction/breastcancer.csv")
dim(df)
names(df)
str(df)
summary(df)
any(is.na(df))
colnames(df)
colSums(is.na(df))
# Replacing the values of these columns by the mean
df$radius_mean[is.na(df$radius_mean)] <- mean(df$radius_mean, na.rm = TRUE)
df$concave.points_mean[is.na(df$concave.points_mean)] <- mean(df$concave.points_mean,na.rm = TRUE)
df$smoothness_se[is.na(df$smoothness_se)] <- mean(df$smoothness_se, na.rm = TRUE)
df$texture_worst[is.na(df$texture_worst)] <- mean(df$texture_worst, na.rm = TRUE)
df$symmetry_worst[is.na(df$symmetry_worst)] <- mean(df$symmetry_worst, na.rm = TRUE)
# checking again if there is any NA values in the numeric columns
colSums(is.na(df))
# the only columns that is character is "diagnosis"
# this is a function to find the mode in a column
calc_mode <- function(x)
{
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
# replacing the NA values by the mode
df <- df %>%
mutate(diagnosis = if_else(is.na(diagnosis),
calc_mode(diagnosis),
diagnosis))
# checking again for NA values in the data.
any(is.na(df)) #Should Be FALSE
df_c <-df
df_c
skim_without_charts(df_c)
df_c %>% count(diagnosis)
# setting diagnosis column to factor for plotting and modeling
df_c$diagnosis <- as.factor(df_c$diagnosis)
# plotting the bar plot
ggplot(df_c, aes(x=diagnosis, fill= diagnosis)) +
geom_bar(stat="count") +
theme_classic() +
scale_y_continuous(breaks = seq(0, 400, by = 25)) +
labs(title="Distribution of Diagnosis")
fig2 <- df_c[c("radius_mean", "diagnosis")]
# plotting the box plot
ggplot(fig2, aes(diagnosis, radius_mean, fill = diagnosis)) +
geom_boxplot()+
labs(col="Type of The Tumor") + ylab("lobes radius mean") +
labs(title="Distribution of diagnosis")+
# changing the color of the boxplots
scale_fill_manual(values = c( "dodgerblue1","red3"))
ggplot(df_c, aes(radius_mean, concavity_mean)) +
geom_point(aes(color = diagnosis)) +
labs(title = "Radious mean Vs Concavity mean",
y = "Mean of Concavity", x = "Radius of Lobes",
col="Type of The Tumor")+
scale_colour_manual(labels = c("Benign", "Malignant"),
values = c("dodgerblue1","red2")) +
theme_bw()
#Machine Learning Analysis
library(kernlab)
#Splitting the data into training set and test set
# setting the independent variable to factor so that we can be able to fit it to the classification model
df_c$diagnosis <- as.factor(df_c$diagnosis)
df_c <- df_c[,-1] #Removing ID column as it does not have correlation
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_c$diagnosis, p=split, list=FALSE)
data_train <- df_c[ trainIndex,]
data_test <- df_c[-trainIndex,]
#Building a Cross validation Object
# Remove rows with missing values
data_train <- na.omit(data_train)
#use train() and trainControl()
train_control <- trainControl(method="cv", number=10)
#Building a K-Nearest Neighbor (KNN) Classification Model
# train the model
model1 <- train(diagnosis~., data=data_train, trControl=train_control,
method="knn")
install.packages("caTools")
library(ggplot2)
library(kableExtra)
library(dplyr)
library(caTools)
setwd("~/Downloads/Fall23/Stat5301_Applied Regression/Project/Admission")
EDA
#EDA
#Dataset consists of 400 observations and 8 features.
# data load and remove serual no.
graduate <- read.csv("~/Downloads/Fall23/Stat5301_Applied Regression/Project/Admission/Admission_Predict.csv") %>% dplyr::select(-Serial.No.)
# data overview
glimpse(graduate)
## Rows: 400
#Change the column name
names(graduate)[1] <- "GRE_Score"
names(graduate)[2] <- "TOEFL_Score"
names(graduate)[3] <- "University_Rating"
names(graduate)[8] <- "Admission_Chance"
#Checking for null values.
# null value check
graduate[!complete.cases(graduate),]
#Change the column name
names(graduate)[1] <- "GRE_Score"
names(graduate)[2] <- "TOEFL_Score"
names(graduate)[3] <- "University_Rating"
names(graduate)[8] <- "Admission_Chance"
#Checking for null values.
# null value check
graduate[!complete.cases(graduate),]
#Change the column name
names(graduate)[1] <- "GRE_Score"
names(graduate)[2] <- "TOEFL_Score"
names(graduate)[3] <- "University_Rating"
names(graduate)[8] <- "Admission_Chance"
#Checking for null values.
# null value check
graduate[!complete.cases(graduate),]
#to see relationship between variables
plot(graduate, col="yellow")
#Change the column name
names(graduate)[1] <- "GRE_Score"
names(graduate)[2] <- "TOEFL_Score"
names(graduate)[3] <- "University_Rating"
names(graduate)[8] <- "Admission_Chance"
#Checking for null values.
# null value check
graduate[!complete.cases(graduate),]
#to see relationship between variables
plot(graduate, col="lightblue")
library(GGally)
library(GGally)
library(ggplot2)
library(GGally)
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(graduate,lower = list(continuous = wrap('points', colour = "purple")),
diag = list(continuous = wrap("barDiag", colour = "violet"))
)
set.seed(2)
sample = sample.split(graduate$Admission_Chance, SplitRatio = 0.70)
train = subset(graduate, sample == TRUE)
test = subset(graduate, sample == FALSE)
print(dim(train))
set.seed(2)
sample = sample.split(graduate$Admission_Chance, SplitRatio = 0.70)
train = subset(graduate, sample == TRUE)
test = subset(graduate, sample == FALSE)
print(dim(train))
print(dim(test))
model_mlr <- lm(Admission_Chance ~ ., data = graduate)
summary(model_mlr)
hist(resid(model_mlr), col = 'skyblue', main = "Residual Disribution", xlab = "Residuals")
plot(model_mlr)
plot(model_mlr)
Predict <- predict(model_mlr, test)
test$Predict <- ifelse(Predict < 0.6, "0", "1")
kable(test[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
GRE_Score
Predict <- predict(model_mlr, test)
test$Predict <- ifelse(Predict < 0.6, "0", "1")
kable(test[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")
library(tidyverse) #For tidying the data
library(skimr) #It is designed to provide summary statistics about variables in data frames
library(ggplot2) #For plotting
library(RColorBrewer) #For aesthetics
library(tidymodels) #For splitting the data and training
library(caret) #For machine learning
library(party) #The core of the package is ctree()
library(yardstick) #For plotting the confusion matrix
df <- read.csv("/Users/sabrinamobassirin/Downloads/Fall23/Stat5301_Applied Regression/Project/Brest_Cancer_prediction/breastcancer.csv")
dim(df)
names(df)
str(df)
summary(df)
any(is.na(df))
colnames(df)
colSums(is.na(df))
# Replacing the values of these columns by the mean
df$radius_mean[is.na(df$radius_mean)] <- mean(df$radius_mean, na.rm = TRUE)
df$concave.points_mean[is.na(df$concave.points_mean)] <- mean(df$concave.points_mean,na.rm = TRUE)
df$smoothness_se[is.na(df$smoothness_se)] <- mean(df$smoothness_se, na.rm = TRUE)
df$texture_worst[is.na(df$texture_worst)] <- mean(df$texture_worst, na.rm = TRUE)
df$symmetry_worst[is.na(df$symmetry_worst)] <- mean(df$symmetry_worst, na.rm = TRUE)
# checking again if there is any NA values in the numeric columns
colSums(is.na(df))
# the only columns that is character is "diagnosis"
# this is a function to find the mode in a column
calc_mode <- function(x)
{
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
# replacing the NA values by the mode
df <- df %>%
mutate(diagnosis = if_else(is.na(diagnosis),
calc_mode(diagnosis),
diagnosis))
# checking again for NA values in the data.
any(is.na(df)) #Should Be FALSE
df_c <-df
df_c
skim_without_charts(df_c)
df_c %>% count(diagnosis)
# setting diagnosis column to factor for plotting and modeling
df_c$diagnosis <- as.factor(df_c$diagnosis)
# plotting the bar plot
ggplot(df_c, aes(x=diagnosis, fill= diagnosis)) +
geom_bar(stat="count") +
theme_classic() +
scale_y_continuous(breaks = seq(0, 400, by = 25)) +
labs(title="Distribution of Diagnosis")
fig2 <- df_c[c("radius_mean", "diagnosis")]
# plotting the box plot
ggplot(fig2, aes(diagnosis, radius_mean, fill = diagnosis)) +
geom_boxplot()+
labs(col="Type of The Tumor") + ylab("lobes radius mean") +
labs(title="Distribution of diagnosis")+
# changing the color of the boxplots
scale_fill_manual(values = c( "dodgerblue1","red3"))
ggplot(df_c, aes(radius_mean, concavity_mean)) +
geom_point(aes(color = diagnosis)) +
labs(title = "Radious mean Vs Concavity mean",
y = "Mean of Concavity", x = "Radius of Lobes",
col="Type of The Tumor")+
scale_colour_manual(labels = c("Benign", "Malignant"),
values = c("dodgerblue1","red2")) +
theme_bw()
# Picking the columns that are highly correlated with the mean of lobes radius
df_R <- df[c("perimeter_mean", "area_mean" , "concave.points_mean", "radius_worst",
"radius_worst", "perimeter_worst", "area_worst", "radius_mean" )]
# Picking the columns that are highly correlated with the mean of lobes radius
df_R <- df[c("perimeter_mean", "area_mean" , "concave.points_mean", "radius_worst",
"radius_worst", "perimeter_worst", "area_worst", "radius_mean" )]
cor(df_R)[,8]
ggplot(df_R, aes(radius_mean, area_mean))+
geom_point(aes(alpha = 0.1))+ geom_smooth(method = "lm") +
labs(title = "The mean of lobes radius Vs the mean of the lobes area",
y = "The mean of lobes areas", x = "The mean of lobes radius") +theme_bw()
ggplot(df_R, aes(radius_mean, perimeter_mean))+
geom_point(aes(alpha = 0.1))+ geom_smooth(method = "lm") +
labs(title = "The mean of lobes radius Vs the mean of the lobes perimeter",
y = "The mean of lobes perimeter", x = "The mean of lobes radius") +theme_bw()
#Splitting The Data to Training and Testing Sets
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_R$radius_mean, p=split, list=FALSE)
data_train_R <- df_R[ trainIndex,]
data_test_R <- df_R[-trainIndex,]
#Training The Multiple Linear Regression Model
model3 <- lm(radius_mean ~ perimeter_mean+area_mean+concave.points_mean+radius_worst+radius_worst+
perimeter_worst+area_worst,data = data_train_R)
#Examining The Coefficient Table
summary(model3)$coefficient
#Splitting The Data to Training and Testing Sets
split=0.80 # define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df_R$radius_mean, p=split, list=FALSE)
data_train_R <- df_R[ trainIndex,]
data_test_R <- df_R[-trainIndex,]
#Training The Multiple Linear Regression Model
model3 <- lm(radius_mean ~ perimeter_mean+area_mean+concave.points_mean+radius_worst+radius_worst+
perimeter_worst+area_worst,data = data_train_R)
#Examining The Coefficient Table
summary(model3)$coefficient
#Using The Model on The Test Data to Predict The Radius Mean
prediction_R = predict(model3, newdata =data_test_R, interval = "prediction")
head(prediction_R) #Exploring the head of our prediction table
#Scatter Plot for The Actual Values Vs the Predicted Values for Radius Mean
# binding the predicted values with the actual ones
act_pred <- as.data.frame(cbind(data_test_R$radius_mean,prediction_R ))
# taking only the predicted and actual values without the upper and lower boundaries
act_pred <- act_pred[,1:2]
#renaming the columns
act_pred <- act_pred %>% rename( Actual= V1, predicted= fit )
#assigning row names to Null to get rid of the unorganized index
row.names(act_pred) <- NULL
head(act_pred)
####
ggplot(act_pred, aes(Actual, predicted)) +
geom_point(alpha = 0.4) + labs(title = "Predicted Values Vs Actual Values For Radius Mean"
, x = "Actual Values of The Test Data",  "Predicted Values of The Test Data")+ theme_gray()
#Checking The Accuarcy Using The Residual Standard Error (RSE), or Sigma:
# The error rate can be estimated by dividing the RSE by the mean outcome variable:
# the smaller the error the more accurate the model is.
sigma(model3)/mean(data_test_R$radius_mean)
